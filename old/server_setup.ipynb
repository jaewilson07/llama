{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install huggingface_hub transformers llama_index nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from nbdev.showdoc import patch_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def upsert_folder(folder_path: str, debug_prn: bool = False):\n",
    "\n",
    "    folder_path = os.path.dirname(folder_path)\n",
    "\n",
    "\n",
    "    if debug_prn:\n",
    "\n",
    "        print(\n",
    "            {\n",
    "                \"upsert_folder\": os.path.abspath(folder_path),\n",
    "                \"is_exist\": os.path.exists(folder_path),\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "\n",
    "        os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelPath:\n",
    "    install_path: str\n",
    "    cache_path: str = None\n",
    "    embedding_path: str = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.cache_path = self.cache_path or f\"{self.install_path}/cache/\"\n",
    "        self.embedding_path = self.embedding_path or f\"{self.install_path}/embedding/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'install_path': '../../models',\n",
       " 'cache_path': '../../models/cache/',\n",
       " 'embedding_path': '../../models/embedding/'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdict(ModelPath(install_path=\"../../models\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    install_path: str\n",
    "\n",
    "    prompt_style: str = \"llama2\"\n",
    "    llm_hf_repo_id: str = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
    "    llm_tokenizer_name: str = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "    llm_hf_model_file: str = \"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "    embedding_hf_model_name: str = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "    model_paths: ModelPath = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.model_paths = ModelPath(install_path=self.install_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(ModelConfig)\n",
    "def setup_model_folders(self, debug_prn: bool = False):\n",
    "    [\n",
    "        upsert_folder(folder_path, debug_prn=debug_prn)\n",
    "        for folder_path in asdict(self.model_paths).values()\n",
    "    ]\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'upsert_folder': 'c:\\\\Users\\\\jaewi\\\\GitHub\\\\llama\\\\models', 'is_exist': True}\n",
      "{'upsert_folder': 'c:\\\\Users\\\\jaewi\\\\GitHub\\\\llama\\\\models\\\\cache', 'is_exist': True}\n",
      "{'upsert_folder': 'c:\\\\Users\\\\jaewi\\\\GitHub\\\\llama\\\\models\\\\embedding', 'is_exist': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ModelConfig(install_path='./models/', \n",
    "    llm_hf_repo_id  = \"TheBloke/gorilla-openfunctions-v1-GGUF\",\n",
    "    llm_hf_model_file  = \"gorilla-openfunctions-v1.Q8_0.gguf\",\n",
    ")\n",
    "\n",
    "config.setup_model_folders( debug_prn = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "@patch_to(ModelConfig)\n",
    "def download_embeddings_model(self):\n",
    "    print(f\"Downloading embedding {self.embedding_hf_model_name}\")\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=self.embedding_hf_model_name,\n",
    "        cache_dir=self.model_paths.cache_path,\n",
    "        local_dir=self.model_paths.embedding_path,\n",
    "    )\n",
    "    print(f\"Embedding model downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading embedding BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04eed36923444c4bba520d2562b659fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaewi\\GitHub\\llama\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jaewi\\GitHub\\llama\\models. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "config.download_embeddings_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(ModelConfig)\n",
    "def download_llm_model(self, resume_download: bool = True):\n",
    "    \"\"\"Download LLM and create a symlink to the model file\"\"\"\n",
    "    print(f\"Downloading LLM {self.llm_hf_model_file}\")\n",
    "\n",
    "    hf_hub_download(\n",
    "        repo_id=self.llm_hf_repo_id,\n",
    "        filename=self.llm_hf_model_file,\n",
    "        cache_dir=self.model_paths.cache_path,\n",
    "        local_dir=self.model_paths.install_path,\n",
    "        resume_download=resume_download,\n",
    "    )\n",
    "    print(\"LLM model downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LLM gorilla-openfunctions-v1.Q8_0.gguf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1779949be444053bc9b0d47bcc9d594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gorilla-openfunctions-v1.Q8_0.gguf:   0%|          | 0.00/7.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM model downloaded!\n"
     ]
    }
   ],
   "source": [
    "config.download_llm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(ModelConfig)\n",
    "def download_tokenizer(self):\n",
    "    print(f\"Downloading tokenizer {self.llm_tokenizer_name}\")\n",
    "\n",
    "    AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=self.llm_tokenizer_name,\n",
    "        cache_dir=self.model_paths.cache_path,\n",
    "    )\n",
    "    print(\"Tokenizer downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer mistralai/Mistral-7B-Instruct-v0.1\n",
      "Tokenizer downloaded!\n"
     ]
    }
   ],
   "source": [
    "config.download_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(ModelConfig)\n",
    "def setup(self):\n",
    "    self.download_embeddings_model()\n",
    "    self.download_llm_model()\n",
    "    self.download_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading embedding BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e084ad5d546949cfb0b56ba5c35206a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model downloaded\n",
      "Downloading LLM gorilla-openfunctions-v1.Q8_0.gguf\n",
      "LLM model downloaded!\n",
      "Downloading tokenizer mistralai/Mistral-7B-Instruct-v0.1\n",
      "Tokenizer downloaded!\n"
     ]
    }
   ],
   "source": [
    "config.setup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
